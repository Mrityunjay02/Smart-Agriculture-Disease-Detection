{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plant Disease Classification using Deep Learning\n",
    "\n",
    "This notebook performs image classification on plant disease dataset using CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Constants and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 224\n",
    "CHANNELS = 3\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "DATASET_PATH = 'D:/ML/Dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 14\n",
      "Classes: ['Pepper__bell___Bacterial_spot', 'Pepper__bell___healthy', 'Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy', 'Tomato_Bacterial_spot', 'Tomato_Early_blight', 'Tomato_Late_blight', 'Tomato_Leaf_Mold', 'Tomato_Spider_mites_Two_spotted_spider_mite', 'Tomato__Target_Spot', 'Tomato__Tomato_YellowLeaf__Curl_Virus', 'Tomato__Tomato_mosaic_virus', 'Tomato_healthy']\n"
     ]
    }
   ],
   "source": [
    "# Get class names from directory structure\n",
    "class_names = sorted([name for name in os.listdir(DATASET_PATH) if os.path.isdir(os.path.join(DATASET_PATH, name))])\n",
    "num_classes = len(class_names)\n",
    "print(f'Number of classes: {num_classes}')\n",
    "print('Classes:', class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15099 images belonging to 14 classes.\n",
      "Found 3768 images belonging to 14 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create data generator with augmentation for training\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Load training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    DATASET_PATH,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Load validation data\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    DATASET_PATH,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 222, 222, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 111, 111, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 111, 111, 32)     128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 109, 109, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 54, 54, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 54, 54, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 52, 52, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 26, 26, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 26, 26, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 24, 24, 256)       295168    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 12, 12, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 12, 12, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 36864)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               18874880  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 14)                7182      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,272,398\n",
      "Trainable params: 19,271,438\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create CNN model\n",
    "model = models.Sequential([\n",
    "    # First Convolutional Block\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, CHANNELS)),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.BatchNormalization(),\n",
    "    \n",
    "    # Second Convolutional Block\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.BatchNormalization(),\n",
    "    \n",
    "    # Third Convolutional Block\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.BatchNormalization(),\n",
    "    \n",
    "    # Fourth Convolutional Block\n",
    "    layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.BatchNormalization(),\n",
    "    \n",
    "    # Flatten and Dense Layers\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "472/472 [==============================] - 801s 2s/step - loss: 1.0678 - accuracy: 0.6633 - val_loss: 1.6951 - val_accuracy: 0.6237\n",
      "Epoch 2/20\n",
      "472/472 [==============================] - 816s 2s/step - loss: 0.9764 - accuracy: 0.6879 - val_loss: 1.0881 - val_accuracy: 0.6661\n",
      "Epoch 3/20\n",
      "472/472 [==============================] - 827s 2s/step - loss: 0.9193 - accuracy: 0.7099 - val_loss: 0.7993 - val_accuracy: 0.7694\n",
      "Epoch 4/20\n",
      "472/472 [==============================] - 807s 2s/step - loss: 0.8094 - accuracy: 0.7457 - val_loss: 1.1714 - val_accuracy: 0.7553\n",
      "Epoch 5/20\n",
      "472/472 [==============================] - 793s 2s/step - loss: 0.7657 - accuracy: 0.7569 - val_loss: 0.5252 - val_accuracy: 0.8609\n",
      "Epoch 6/20\n",
      "472/472 [==============================] - 817s 2s/step - loss: 0.6947 - accuracy: 0.7806 - val_loss: 0.5180 - val_accuracy: 0.8572\n",
      "Epoch 7/20\n",
      "472/472 [==============================] - 828s 2s/step - loss: 0.6559 - accuracy: 0.7922 - val_loss: 2.9594 - val_accuracy: 0.4756\n",
      "Epoch 8/20\n",
      "472/472 [==============================] - 822s 2s/step - loss: 0.6230 - accuracy: 0.8063 - val_loss: 0.7732 - val_accuracy: 0.7585\n",
      "Epoch 9/20\n",
      "472/472 [==============================] - 713s 2s/step - loss: 0.5674 - accuracy: 0.8279 - val_loss: 0.4374 - val_accuracy: 0.8684\n",
      "Epoch 10/20\n",
      "472/472 [==============================] - 710s 2s/step - loss: 0.5488 - accuracy: 0.8336 - val_loss: 1.1960 - val_accuracy: 0.7142\n",
      "Epoch 11/20\n",
      "472/472 [==============================] - 832s 2s/step - loss: 0.5038 - accuracy: 0.8442 - val_loss: 2.9454 - val_accuracy: 0.4615\n",
      "Epoch 12/20\n",
      "472/472 [==============================] - 809s 2s/step - loss: 0.4594 - accuracy: 0.8610 - val_loss: 1.0878 - val_accuracy: 0.7113\n",
      "Epoch 13/20\n",
      "472/472 [==============================] - 800s 2s/step - loss: 0.4490 - accuracy: 0.8626 - val_loss: 1.3392 - val_accuracy: 0.7184\n",
      "Epoch 14/20\n",
      "472/472 [==============================] - 785s 2s/step - loss: 0.4301 - accuracy: 0.8676 - val_loss: 0.7819 - val_accuracy: 0.7588\n",
      "Epoch 15/20\n",
      "472/472 [==============================] - 806s 2s/step - loss: 0.4123 - accuracy: 0.8744 - val_loss: 0.8769 - val_accuracy: 0.8126\n",
      "Epoch 16/20\n",
      "472/472 [==============================] - 857s 2s/step - loss: 0.3781 - accuracy: 0.8854 - val_loss: 0.9174 - val_accuracy: 0.7816\n",
      "Epoch 17/20\n",
      "472/472 [==============================] - 869s 2s/step - loss: 0.3661 - accuracy: 0.8909 - val_loss: 3.0939 - val_accuracy: 0.6521\n",
      "Epoch 18/20\n",
      "105/472 [=====>........................] - ETA: 11:08 - loss: 0.3339 - accuracy: 0.8997"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Plot training & validation accuracy\u001b[39;00m\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeUAAAFlCAYAAADVgPC6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAaLUlEQVR4nO3db2zV5f3/8ddpS0+R7RwDaC1Qa3GgVSKONlTKGqPTGiAYEhdqXCw6TGzUIXQ4qV1EiEmji2SitP6hhZgU1sm/cKNDzo0Nyp/9oWuNsU00wmzR1qY1nhZ1Rcr1vcGP8/PYonwO55R36/ORnBvn2vU55zpXuj33Oed8OD7nnBMAALjski73AgAAwDlEGQAAI4gyAABGEGUAAIwgygAAGEGUAQAwgigDAGAEUQYAwAiiDACAEUQZAAAjPEf54MGDWrx4saZMmSKfz6c9e/b84DEHDhxQbm6u0tLSNH36dL322muxrBUAgDHNc5S//PJLzZ49W6+++upFzT9x4oQWLlyowsJCNTc365lnntGKFSu0c+dOz4sFAGAs813KD1L4fD7t3r1bS5YsueCcp59+Wnv37lVbW1tkrLS0VO+++66OHj0a61MDADDmpCT6CY4ePaqioqKosXvuuUc1NTX65ptvNG7cuCHHDAwMaGBgIHL/7Nmz+vzzzzVp0iT5fL5ELxkAgB/knFN/f7+mTJmipKT4fEUr4VHu6upSenp61Fh6errOnDmjnp4eZWRkDDmmsrJS69atS/TSAAC4ZB0dHZo2bVpcHivhUZY05Oz2/DvmFzrrLS8vV1lZWeR+OBzWtddeq46ODgUCgcQtFACAi9TX16fMzEz99Kc/jdtjJjzK11xzjbq6uqLGuru7lZKSokmTJg17jN/vl9/vHzIeCASIMgDAlHh+rJrw65TnzZunUCgUNbZ//37l5eUN+3kyAAA/Vp6jfOrUKbW0tKilpUXSuUueWlpa1N7eLuncW88lJSWR+aWlpfr4449VVlamtrY21dbWqqamRqtXr47PKwAAYIzw/Pb1sWPHdMcdd0Tun//sd9myZdq6das6OzsjgZak7OxsNTQ0aNWqVdq0aZOmTJmijRs36r777ovD8gEAGDsu6TrlkdLX16dgMKhwOMxnygAAExLRJv7tawAAjCDKAAAYQZQBADCCKAMAYARRBgDACKIMAIARRBkAACOIMgAARhBlAACMIMoAABhBlAEAMIIoAwBgBFEGAMAIogwAgBFEGQAAI4gyAABGEGUAAIwgygAAGEGUAQAwgigDAGAEUQYAwAiiDACAEUQZAAAjiDIAAEYQZQAAjCDKAAAYQZQBADCCKAMAYARRBgDACKIMAIARRBkAACOIMgAARhBlAACMIMoAABhBlAEAMIIoAwBgBFEGAMAIogwAgBFEGQAAI4gyAABGEGUAAIwgygAAGEGUAQAwgigDAGAEUQYAwAiiDACAEUQZAAAjiDIAAEYQZQAAjCDKAAAYQZQBADAipihXVVUpOztbaWlpys3NVWNj4/fOr6ur0+zZs3XFFVcoIyNDDz/8sHp7e2NaMAAAY5XnKNfX12vlypWqqKhQc3OzCgsLtWDBArW3tw87/9ChQyopKdHy5cv1/vvv6+2339a///1vPfLII5e8eAAAxhLPUd6wYYOWL1+uRx55RDk5OfrTn/6kzMxMVVdXDzv/H//4h6677jqtWLFC2dnZ+sUvfqFHH31Ux44du+TFAwAwlniK8unTp9XU1KSioqKo8aKiIh05cmTYYwoKCnTy5Ek1NDTIOafPPvtMO3bs0KJFiy74PAMDA+rr64u6AQAw1nmKck9PjwYHB5Wenh41np6erq6urmGPKSgoUF1dnYqLi5WamqprrrlGV155pV555ZULPk9lZaWCwWDklpmZ6WWZAACMSjF90cvn80Xdd84NGTuvtbVVK1as0LPPPqumpibt27dPJ06cUGlp6QUfv7y8XOFwOHLr6OiIZZkAAIwqKV4mT548WcnJyUPOiru7u4ecPZ9XWVmp+fPn66mnnpIk3XLLLZowYYIKCwv1/PPPKyMjY8gxfr9ffr/fy9IAABj1PJ0pp6amKjc3V6FQKGo8FAqpoKBg2GO++uorJSVFP01ycrKkc2fYAADgHM9vX5eVlWnz5s2qra1VW1ubVq1apfb29sjb0eXl5SopKYnMX7x4sXbt2qXq6modP35chw8f1ooVKzR37lxNmTIlfq8EAIBRztPb15JUXFys3t5erV+/Xp2dnZo1a5YaGhqUlZUlSers7Iy6Zvmhhx5Sf3+/Xn31Vf3ud7/TlVdeqTvvvFMvvPBC/F4FAABjgM+NgveQ+/r6FAwGFQ6HFQgELvdyAABISJv4t68BADCCKAMAYARRBgDACKIMAIARRBkAACOIMgAARhBlAACMIMoAABhBlAEAMIIoAwBgBFEGAMAIogwAgBFEGQAAI4gyAABGEGUAAIwgygAAGEGUAQAwgigDAGAEUQYAwAiiDACAEUQZAAAjiDIAAEYQZQAAjCDKAAAYQZQBADCCKAMAYARRBgDACKIMAIARRBkAACOIMgAARhBlAACMIMoAABhBlAEAMIIoAwBgBFEGAMAIogwAgBFEGQAAI4gyAABGEGUAAIwgygAAGEGUAQAwgigDAGAEUQYAwAiiDACAEUQZAAAjiDIAAEYQZQAAjCDKAAAYQZQBADCCKAMAYERMUa6qqlJ2drbS0tKUm5urxsbG750/MDCgiooKZWVlye/36/rrr1dtbW1MCwYAYKxK8XpAfX29Vq5cqaqqKs2fP1+vv/66FixYoNbWVl177bXDHrN06VJ99tlnqqmp0c9+9jN1d3frzJkzl7x4AADGEp9zznk5ID8/X3PmzFF1dXVkLCcnR0uWLFFlZeWQ+fv27dP999+v48ePa+LEiTEtsq+vT8FgUOFwWIFAIKbHAAAgnhLRJk9vX58+fVpNTU0qKiqKGi8qKtKRI0eGPWbv3r3Ky8vTiy++qKlTp2rmzJlavXq1vv766ws+z8DAgPr6+qJuAACMdZ7evu7p6dHg4KDS09OjxtPT09XV1TXsMcePH9ehQ4eUlpam3bt3q6enR4899pg+//zzC36uXFlZqXXr1nlZGgAAo15MX/Ty+XxR951zQ8bOO3v2rHw+n+rq6jR37lwtXLhQGzZs0NatWy94tlxeXq5wOBy5dXR0xLJMAABGFU9nypMnT1ZycvKQs+Lu7u4hZ8/nZWRkaOrUqQoGg5GxnJwcOed08uRJzZgxY8gxfr9ffr/fy9IAABj1PJ0pp6amKjc3V6FQKGo8FAqpoKBg2GPmz5+vTz/9VKdOnYqMffDBB0pKStK0adNiWDIAAGOT57evy8rKtHnzZtXW1qqtrU2rVq1Se3u7SktLJZ1767mkpCQy/4EHHtCkSZP08MMPq7W1VQcPHtRTTz2l3/zmNxo/fnz8XgkAAKOc5+uUi4uL1dvbq/Xr16uzs1OzZs1SQ0ODsrKyJEmdnZ1qb2+PzP/JT36iUCik3/72t8rLy9OkSZO0dOlSPf/88/F7FQAAjAGer1O+HLhOGQBgzWW/ThkAACQOUQYAwAiiDACAEUQZAAAjiDIAAEYQZQAAjCDKAAAYQZQBADCCKAMAYARRBgDACKIMAIARRBkAACOIMgAARhBlAACMIMoAABhBlAEAMIIoAwBgBFEGAMAIogwAgBFEGQAAI4gyAABGEGUAAIwgygAAGEGUAQAwgigDAGAEUQYAwAiiDACAEUQZAAAjiDIAAEYQZQAAjCDKAAAYQZQBADCCKAMAYARRBgDACKIMAIARRBkAACOIMgAARhBlAACMIMoAABhBlAEAMIIoAwBgBFEGAMAIogwAgBFEGQAAI4gyAABGEGUAAIwgygAAGEGUAQAwgigDAGAEUQYAwAiiDACAETFFuaqqStnZ2UpLS1Nubq4aGxsv6rjDhw8rJSVFt956ayxPCwDAmOY5yvX19Vq5cqUqKirU3NyswsJCLViwQO3t7d97XDgcVklJiX75y1/GvFgAAMYyn3POeTkgPz9fc+bMUXV1dWQsJydHS5YsUWVl5QWPu//++zVjxgwlJydrz549amlpuejn7OvrUzAYVDgcViAQ8LJcAAASIhFt8nSmfPr0aTU1NamoqChqvKioSEeOHLngcVu2bNFHH32ktWvXXtTzDAwMqK+vL+oGAMBY5ynKPT09GhwcVHp6etR4enq6urq6hj3mww8/1Jo1a1RXV6eUlJSLep7KykoFg8HILTMz08syAQAYlWL6opfP54u675wbMiZJg4ODeuCBB7Ru3TrNnDnzoh+/vLxc4XA4cuvo6IhlmQAAjCoXd+r6/0yePFnJyclDzoq7u7uHnD1LUn9/v44dO6bm5mY98cQTkqSzZ8/KOaeUlBTt379fd95555Dj/H6//H6/l6UBADDqeTpTTk1NVW5urkKhUNR4KBRSQUHBkPmBQEDvvfeeWlpaIrfS0lLdcMMNamlpUX5+/qWtHgCAMcTTmbIklZWV6cEHH1ReXp7mzZunN954Q+3t7SotLZV07q3nTz75RG+99ZaSkpI0a9asqOOvvvpqpaWlDRkHAODHznOUi4uL1dvbq/Xr16uzs1OzZs1SQ0ODsrKyJEmdnZ0/eM0yAAAYyvN1ypcD1ykDAKy57NcpAwCAxCHKAAAYQZQBADCCKAMAYARRBgDACKIMAIARRBkAACOIMgAARhBlAACMIMoAABhBlAEAMIIoAwBgBFEGAMAIogwAgBFEGQAAI4gyAABGEGUAAIwgygAAGEGUAQAwgigDAGAEUQYAwAiiDACAEUQZAAAjiDIAAEYQZQAAjCDKAAAYQZQBADCCKAMAYARRBgDACKIMAIARRBkAACOIMgAARhBlAACMIMoAABhBlAEAMIIoAwBgBFEGAMAIogwAgBFEGQAAI4gyAABGEGUAAIwgygAAGEGUAQAwgigDAGAEUQYAwAiiDACAEUQZAAAjiDIAAEYQZQAAjCDKAAAYEVOUq6qqlJ2drbS0NOXm5qqxsfGCc3ft2qW7775bV111lQKBgObNm6d33nkn5gUDADBWeY5yfX29Vq5cqYqKCjU3N6uwsFALFixQe3v7sPMPHjyou+++Ww0NDWpqatIdd9yhxYsXq7m5+ZIXDwDAWOJzzjkvB+Tn52vOnDmqrq6OjOXk5GjJkiWqrKy8qMe4+eabVVxcrGefffai5vf19SkYDCocDisQCHhZLgAACZGINnk6Uz59+rSamppUVFQUNV5UVKQjR45c1GOcPXtW/f39mjhxopenBgBgzEvxMrmnp0eDg4NKT0+PGk9PT1dXV9dFPcZLL72kL7/8UkuXLr3gnIGBAQ0MDETu9/X1eVkmAACjUkxf9PL5fFH3nXNDxoazfft2Pffcc6qvr9fVV199wXmVlZUKBoORW2ZmZizLBABgVPEU5cmTJys5OXnIWXF3d/eQs+fvqq+v1/Lly/WXv/xFd9111/fOLS8vVzgcjtw6Ojq8LBMAgFHJU5RTU1OVm5urUCgUNR4KhVRQUHDB47Zv366HHnpI27Zt06JFi37wefx+vwKBQNQNAICxztNnypJUVlamBx98UHl5eZo3b57eeOMNtbe3q7S0VNK5s9xPPvlEb731lqRzQS4pKdHLL7+s2267LXKWPX78eAWDwTi+FAAARjfPUS4uLlZvb6/Wr1+vzs5OzZo1Sw0NDcrKypIkdXZ2Rl2z/Prrr+vMmTN6/PHH9fjjj0fGly1bpq1bt176KwAAYIzwfJ3y5cB1ygAAay77dcoAACBxiDIAAEYQZQAAjCDKAAAYQZQBADCCKAMAYARRBgDACKIMAIARRBkAACOIMgAARhBlAACMIMoAABhBlAEAMIIoAwBgBFEGAMAIogwAgBFEGQAAI4gyAABGEGUAAIwgygAAGEGUAQAwgigDAGAEUQYAwAiiDACAEUQZAAAjiDIAAEYQZQAAjCDKAAAYQZQBADCCKAMAYARRBgDACKIMAIARRBkAACOIMgAARhBlAACMIMoAABhBlAEAMIIoAwBgBFEGAMAIogwAgBFEGQAAI4gyAABGEGUAAIwgygAAGEGUAQAwgigDAGAEUQYAwAiiDACAEUQZAAAjiDIAAEYQZQAAjIgpylVVVcrOzlZaWppyc3PV2Nj4vfMPHDig3NxcpaWlafr06XrttddiWiwAAGOZ5yjX19dr5cqVqqioUHNzswoLC7VgwQK1t7cPO//EiRNauHChCgsL1dzcrGeeeUYrVqzQzp07L3nxAACMJT7nnPNyQH5+vubMmaPq6urIWE5OjpYsWaLKysoh859++mnt3btXbW1tkbHS0lK9++67Onr06EU9Z19fn4LBoMLhsAKBgJflAgCQEIloU4qXyadPn1ZTU5PWrFkTNV5UVKQjR44Me8zRo0dVVFQUNXbPPfeopqZG33zzjcaNGzfkmIGBAQ0MDETuh8NhSec2AAAAC843yeO57ffyFOWenh4NDg4qPT09ajw9PV1dXV3DHtPV1TXs/DNnzqinp0cZGRlDjqmsrNS6deuGjGdmZnpZLgAACdfb26tgMBiXx/IU5fN8Pl/UfefckLEfmj/c+Hnl5eUqKyuL3P/iiy+UlZWl9vb2uL3wH7O+vj5lZmaqo6ODjwPihD2NL/Yz/tjT+AuHw7r22ms1ceLEuD2mpyhPnjxZycnJQ86Ku7u7h5wNn3fNNdcMOz8lJUWTJk0a9hi/3y+/3z9kPBgM8scUR4FAgP2MM/Y0vtjP+GNP4y8pKX5XF3t6pNTUVOXm5ioUCkWNh0IhFRQUDHvMvHnzhszfv3+/8vLyhv08GQCAHyvPeS8rK9PmzZtVW1urtrY2rVq1Su3t7SotLZV07q3nkpKSyPzS0lJ9/PHHKisrU1tbm2pra1VTU6PVq1fH71UAADAGeP5Mubi4WL29vVq/fr06Ozs1a9YsNTQ0KCsrS5LU2dkZdc1ydna2GhoatGrVKm3atElTpkzRxo0bdd999130c/r9fq1du3bYt7ThHfsZf+xpfLGf8ceexl8i9tTzdcoAACAx+LevAQAwgigDAGAEUQYAwAiiDACAEWaizM9BxpeX/dy1a5fuvvtuXXXVVQoEApo3b57eeeedEVzt6OD1b/S8w4cPKyUlRbfeemtiFzjKeN3PgYEBVVRUKCsrS36/X9dff71qa2tHaLWjg9c9raur0+zZs3XFFVcoIyNDDz/8sHp7e0dotbYdPHhQixcv1pQpU+Tz+bRnz54fPCYuXXIG/PnPf3bjxo1zb775pmttbXVPPvmkmzBhgvv444+HnX/8+HF3xRVXuCeffNK1tra6N998040bN87t2LFjhFduk9f9fPLJJ90LL7zg/vWvf7kPPvjAlZeXu3Hjxrn//Oc/I7xyu7zu6XlffPGFmz59uisqKnKzZ88emcWOArHs57333uvy8/NdKBRyJ06ccP/85z/d4cOHR3DVtnnd08bGRpeUlORefvlld/z4cdfY2Ohuvvlmt2TJkhFeuU0NDQ2uoqLC7dy500lyu3fv/t758eqSiSjPnTvXlZaWRo3deOONbs2aNcPO//3vf+9uvPHGqLFHH33U3XbbbQlb42jidT+Hc9NNN7l169bFe2mjVqx7Wlxc7P7whz+4tWvXEuVv8bqff/3rX10wGHS9vb0jsbxRyeue/vGPf3TTp0+PGtu4caObNm1awtY4Wl1MlOPVpcv+9vX5n4P87s87xvJzkMeOHdM333yTsLWOBrHs53edPXtW/f39cf1H1kezWPd0y5Yt+uijj7R27dpEL3FUiWU/9+7dq7y8PL344ouaOnWqZs6cqdWrV+vrr78eiSWbF8ueFhQU6OTJk2poaJBzTp999pl27NihRYsWjcSSx5x4dSmmX4mKp5H6Ocgfi1j287teeuklffnll1q6dGkiljjqxLKnH374odasWaPGxkalpFz2/5qZEst+Hj9+XIcOHVJaWpp2796tnp4ePfbYY/r888/5XFmx7WlBQYHq6upUXFys//3vfzpz5ozuvfdevfLKKyOx5DEnXl267GfK5yX65yB/bLzu53nbt2/Xc889p/r6el199dWJWt6odLF7Ojg4qAceeEDr1q3TzJkzR2p5o46Xv9GzZ8/K5/Oprq5Oc+fO1cKFC7VhwwZt3bqVs+Vv8bKnra2tWrFihZ599lk1NTVp3759OnHiROR3DOBdPLp02f8v/Ej9HOSPRSz7eV59fb2WL1+ut99+W3fddVcilzmqeN3T/v5+HTt2TM3NzXriiScknYuKc04pKSnav3+/7rzzzhFZu0Wx/I1mZGRo6tSpUb+nnpOTI+ecTp48qRkzZiR0zdbFsqeVlZWaP3++nnrqKUnSLbfcogkTJqiwsFDPP//8j/odx1jEq0uX/UyZn4OMr1j2Uzp3hvzQQw9p27ZtfKb0HV73NBAI6L333lNLS0vkVlpaqhtuuEEtLS3Kz88fqaWbFMvf6Pz58/Xpp5/q1KlTkbEPPvhASUlJmjZtWkLXOxrEsqdfffXVkN8BTk5OlvT/z/Bw8eLWJU9fC0uQ81/lr6mpca2trW7lypVuwoQJ7r///a9zzrk1a9a4Bx98MDL//FfPV61a5VpbW11NTQ2XRH2L1/3ctm2bS0lJcZs2bXKdnZ2R2xdffHG5XoI5Xvf0u/j2dTSv+9nf3++mTZvmfvWrX7n333/fHThwwM2YMcM98sgjl+slmON1T7ds2eJSUlJcVVWV++ijj9yhQ4dcXl6emzt37uV6Cab09/e75uZm19zc7CS5DRs2uObm5sglZonqkokoO+fcpk2bXFZWlktNTXVz5sxxBw4ciPxny5Ytc7fffnvU/L///e/u5z//uUtNTXXXXXedq66uHuEV2+ZlP2+//XYnacht2bJlI79ww7z+jX4bUR7K6362tbW5u+66y40fP95NmzbNlZWVua+++mqEV22b1z3duHGju+mmm9z48eNdRkaG+/Wvf+1Onjw5wqu26W9/+9v3/u9iorrETzcCAGDEZf9MGQAAnEOUAQAwgigDAGAEUQYAwAiiDACAEUQZAAAjiDIAAEYQZQAAjCDKAAAYQZQBADCCKAMAYARRBgDAiP8DGsN9pVbeRcsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot training & validation accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training & validation loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.save('plant_disease_model.h5')\n",
    "print('Model saved successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path):\n",
    "    # Load and preprocess image\n",
    "    img = tf.keras.preprocessing.image.load_img(\n",
    "        image_path,\n",
    "        target_size=(IMG_HEIGHT, IMG_WIDTH)\n",
    "    )\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0)\n",
    "    img_array /= 255.\n",
    "    \n",
    "    # Make prediction\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class = class_names[np.argmax(predictions[0])]\n",
    "    confidence = np.max(predictions[0])\n",
    "    \n",
    "    # Display results\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.imshow(img)\n",
    "    plt.title(f'Predicted: {predicted_class}\\nConfidence: {confidence:.2%}')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
